
<!doctype html>
<html>
<head>
  <meta charset="utf-8">

  <!-- Always force latest IE rendering engine or request Chrome Frame -->
  <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">

  <!-- REPLACE X WITH PRODUCT NAME -->
  <title>PADS 1.1.4 Release Notes | Pivotal HD/PCC/ADS Documentation</title>
  <!-- Local CSS stylesheets -->
  <link href="stylesheets/master.css" media="screen,print" rel="stylesheet" type="text/css" />
  <link href="stylesheets/breadcrumbs.css" media="screen,print" rel="stylesheet" type="text/css" />
  <link href="stylesheets/search.css" media="screen,print" rel="stylesheet" type="text/css" />
  <link href="stylesheets/portal-style.css" media="screen,print" rel="stylesheet" type="text/css" />
  <link href="stylesheets/printable.css" media="print" rel="stylesheet" type="text/css" /> 
  <!-- Confluence HTML stylesheet -->
  <link href="stylesheets/site.css" media="screen,print" rel="stylesheet"  type="text/css" /> 
  <!-- Left-navigation code -->
  <!-- http://www.designchemical.com/lab/jquery-vertical-accordion-menu-plugin/examples/# -->
  <link href="stylesheets/dcaccordion.css" rel="stylesheet" type="text/css" />
  <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.4.2/jquery.min.js" type="text/javascript"></script>
  <script src='javascripts/jquery.cookie.js' type="text/javascript"></script>
  <script src='javascripts/jquery.hoverIntent.minified.js' type="text/javascript"></script>
  <script src='javascripts/jquery.dcjqaccordion.2.7.min.js' type="text/javascript"></script>
  <script type="text/javascript">
                    $(document).ready(function($){
					$('#accordion-1').dcAccordion({
						eventType: 'click',
						autoClose: true,
						saveState: true,
						disableLink: false,
						speed: 'fast',
						classActive: 'test',
						showCount: true
					});
					});
  </script>
  <link href="stylesheets/skins/graphite.css" rel="stylesheet" type="text/css" />
  <link href="stylesheets/skins/grey.css" rel="stylesheet" type="text/css" /> 
  <!-- End left-navigation code -->
  <script src="javascripts/all.js" type="text/javascript"></script>
  <link href='http://www.gopivotal.com/misc/favicon.ico' rel='shortcut icon'>
</head>

<body class="pivotalcf pivotalcf_getstarted pivotalcf_getstarted_index">
  <div class="viewport">
    <div class="mobile-navigation--wrapper mobile-only">
      <div class="navigation-drawer--container">
        <div class="navigation-item-list">
          <div class="navbar-link active">
            <a href="http://gopivotal.com">
              Home
              <i class="icon-chevron-right pull-right"></i>
            </a>
          </div>
          <div class="navbar-link">
            <a href="http://gopivotal.com/paas">
              PaaS
              <i class="icon-chevron-right pull-right"></i>
            </a>
          </div>
          <div class="navbar-link">
            <a href="http://gopivotal.com/big-data">
              Big Data
              <i class="icon-chevron-right pull-right"></i>
            </a>
          </div>
          <div class="navbar-link">
            <a href="http://gopivotal.com/agile">
              Agile
              <i class="icon-chevron-right pull-right"></i>
            </a>
          </div>
          <div class="navbar-link">
            <a href="http://gopivotal.com/support">
              Help &amp; Support
              <i class="icon-chevron-right pull-right"></i>
            </a>
          </div>
          <div class="navbar-link">
            <a href="http://gopivotal.com/products">
              Products
              <i class="icon-chevron-right pull-right"></i>
            </a>
          </div>
          <div class="navbar-link">
            <a href="http://gopivotal.com/solutions">
              Solutions
              <i class="icon-chevron-right pull-right"></i>
            </a>
          </div>
          <div class="navbar-link">
            <a href="http://gopivotal.com/partners">
              Partners
              <i class="icon-chevron-right pull-right"></i>
            </a>
          </div>
        </div>
      </div>
      <div class="mobile-nav">
        <div class="nav-icon js-open-nav-drawer">
          <i class="icon-reorder"></i>
        </div>
        <div class="header-center-icon">
          <a href="http://gopivotal.com">
            <div class="icon icon-pivotal-logo-mobile"></div>
          </a>
        </div>
      </div>
    </div>

    <div class='wrap'>
      <script src="//use.typekit.net/clb0qji.js" type="text/javascript"></script>
      <script type="text/javascript">
          try {
              Typekit.load();
          } catch (e) {
          }
      </script>
      <script type="text/javascript">
          document.domain = "gopivotal.com";
      </script>
      <div id="search-dropdown-box">
        <div class="search-dropdown--container js-search-dropdown">
          <div class="container-fluid">
            <div class="close-menu-large"><img src="http://www.gopivotal.com/sites/all/themes/gopo13/images/icon-close.png" /></div>
            <div class="search-form--container">
              <div class="form-search">
                <div class='gcse-search'></div>
                <script src="http://www.google.com/jsapi" type="text/javascript"></script>
                <script src="javascripts/cse.js" type="text/javascript"></script>
              </div>
            </div>
          </div>
        </div>
      </div>

      <header class="navbar desktop-only" id="nav">
        <div class="navbar-inner">
            <div class="container-fluid">
                <div class="pivotal-logo--container">
                    <a class="pivotal-logo" href="http://gopivotal.com"><span></span></a>
                </div>

                <ul class="nav pull-right">
                    <li class="navbar-link">
                        <a href="http://www.gopivotal.com/paas" id="paas-nav-link">PaaS</a>
                    </li>
                    <li class="navbar-link">
                        <a href="http://www.gopivotal.com/big-data" id="big-data-nav-link">BIG DATA</a>
                    </li>
                    <li class="navbar-link">
                        <a href="http://www.gopivotal.com/agile" id="agile-nav-link">AGILE</a>
                    </li>
                    <li class="navbar-link">
                        <a href="http://www.gopivotal.com/oss" id="oss-nav-link">OSS</a>
                    </li>
                    <li class="nav-search">
                        <a class="js-search-input-open" id="click-to-search"><span></span></a>
                    </li>
                </ul>
            </div>
            <a href="http://www.gopivotal.com/contact">
                <img id="get-started" src="http://www.gopivotal.com/sites/all/themes/gopo13/images/get-started.png">
            </a>
        </div>
      </header>
      <div class="main-wrap">
        <div class="container-fluid">

          <!-- Google CSE Search Box -->
          <div id='docs-search'>
              <gcse:search></gcse:search>
          </div>
          
          <div id='all-docs-link'>
            <a href="http://docs.gopivotal.com/">All Documentation</a>
          </div>
          
          <div class="container">
            <div id="sub-nav" class="nav-container">              
              
              <!-- Collapsible left-navigation-->
			  <ul class="accordion"  id="accordion-1">
				  <!-- REPLACE <li/> NODES-->

                        <li>
                <a href="index.html">Pivotal HD</a>

                            <ul>
                    <li>
                <a href="PHDEnterprise1.1.1ReleaseNotes.html">PHD Enterprise 1.1.1 Release Notes</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="PHDServiceBrokerforPivotalCFv1.0.0.0.html">PHD Service Broker for Pivotal CF v1.0.0.0</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="PHDInstallationandAdministration.html">PHD Installation and Administration</a>

                            <ul>
                    <li>
                <a href="OverviewofPHD.html">Overview of PHD</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="InstallingPHDUsingtheCLI.html">Installing PHD Using the CLI</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="UpgradingPHDUsingtheCLI.html">Upgrading PHD Using the CLI</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="AdministeringPHDUsingtheCLI.html">Administering PHD Using the CLI</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="PHDFAQFrequentlyAskedQuestions.html">PHD FAQ (Frequently Asked Questions)</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="PHDTroubleshooting.html">PHD Troubleshooting</a>

                    </li>
            </ul>
            </li>
            </ul>
                    <ul>
                    <li>
                <a href="StackandToolsReference.html">Stack and Tools Reference</a>

                            <ul>
                    <li>
                <a href="OverviewofApacheStackandPivotalComponents.html">Overview of Apache Stack and Pivotal Components</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="PHD1.1.1Stack-RPMPackage.html">PHD 1.1.1 Stack - RPM Package</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="PHD1.1.1Stack-BinaryPackage.html">PHD 1.1.1 Stack - Binary Package</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="PHDMR11.1Stack-RPMPackage.html">PHD MR1 1.1 Stack - RPM Package</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="PHDMR11.1Stack-BinaryPackage.html">PHD MR1 1.1 Stack - Binary Package</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="PHDStack-OtherComponents.html">PHD Stack - Other Components</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="USSUnifiedStorageSystem.html">USS (Unified Storage System)</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="HVEHadoopVirtualizationExtensions.html">HVE (Hadoop Virtualization Extensions)</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="Security.html">Security</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="ManuallyUpgradingPHDfrom1.1to1.1.1-RPM.html">Manually Upgrading PHD from 1.1 to 1.1.1 - RPM</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="ManuallyUpgradingPHDfrom1.1to1.1.1-Binary.html">Manually Upgrading PHD from 1.1 to 1.1.1 - Binary</a>

                    </li>
            </ul>
            </li>
            </ul>
                    <ul>
                    <li>
                <a href="DataLoaderInstallationandUsage.html">DataLoader Installation and Usage</a>

                            <ul>
                    <li>
                <a href="OverviewofDataLoader.html">Overview of DataLoader</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="InstallingandConfiguringDataLoader.html">Installing and Configuring DataLoader</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="UsingDataLoader.html">Using DataLoader</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="LoadingFilesandPushStreamsintoHAWQUsingPXF.html">Loading Files and Push Streams into HAWQ Using PXF</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="DataLoaderCommandLineInterface.html">DataLoader Command Line Interface</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="DataLoaderCopyStrategyandTransferPolicy.html">DataLoader Copy Strategy and Transfer Policy</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="JobTransferSpecification.html">Job (Transfer) Specification</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="DataStores.html">Data Stores</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="ConfiguringFlumeforDataLoaderPushStreaming.html">Configuring Flume for DataLoader Push Streaming</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="DataLoaderInstallationfromBinaries.html">DataLoader Installation from Binaries</a>

                    </li>
            </ul>
            </li>
            </ul>
            </li>
                        <li>
                <a href="PivotalCommandCenter.html">Pivotal Command Center</a>

                            <ul>
                    <li>
                <a href="PCC2.1.1ReleaseNotes.html">PCC 2.1.1 Release Notes</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="PCCUserGuide.html">PCC User Guide</a>

                            <ul>
                    <li>
                <a href="PCCOverview.html">PCC Overview</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="InstallingPCC.html">Installing PCC</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="UsingPCC.html">Using PCC</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="CreatingaYUMEPELRepository.html">Creating a YUM EPEL Repository</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="CommandLineReference.html">Command Line Reference</a>

                    </li>
            </ul>
            </li>
            </ul>
            </li>
                        <li>
                <a href="PivotalAdvancedDatabaseServices.html">Pivotal Advanced Database Services</a>

                            <ul>
                    <li>
                <a href="PADS1.1.4ReleaseNotes.html">PADS 1.1.4 Release Notes</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="HAWQInstallation.html">HAWQ Installation</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="HAWQAdministration.html">HAWQ Administration</a>

                            <ul>
                    <li>
                <a href="HAWQOverview.html">HAWQ Overview</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="HAWQQueryProcessing.html">HAWQ Query Processing</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="QueryingData.html">Querying Data</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="ConfiguringClientAuthentication.html">Configuring Client Authentication</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="KerberosAuthentication.html">Kerberos Authentication</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="HAWQInputFormatforMapReduce.html">HAWQ InputFormat for MapReduce</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="SQLCommandReference.html">SQL Command Reference</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="ManagementUtilityReference.html">Management Utility Reference</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="ClientUtilityReference.html">Client Utility Reference</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="ServerConfigurationParameters.html">Server Configuration Parameters</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="HAWQEnvironmentVariables.html">HAWQ Environment Variables</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="HAWQDataTypes.html">HAWQ Data Types</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="MADlibReferences.html">MADlib References</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="hawq_toolkitReference.html">hawq_toolkit Reference</a>

                    </li>
            </ul>
            </li>
            </ul>
                    <ul>
                    <li>
                <a href="PivotalExtensionFrameworkPXF.html">Pivotal Extension Framework (PXF)</a>

                            <ul>
                    <li>
                <a href="PXFInstallationandAdministration.html">PXF Installation and Administration</a>

                    </li>
            </ul>
                    <ul>
                    <li>
                <a href="PXFExternalTableandAPIReference.html">PXF External Table and API Reference</a>

                    </li>
            </ul>
            </li>
            </ul>
            </li>
              </ul>        
              
            </div><!--end of sub-nav-->
            <div class="body-container content">

              <!-- Python script replaces main content -->
			  <div id ="main"><h1>PADS 1.1.4 Release Notes</h1><div class="wiki-content group" id="main-content">
<h1 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-WelcometoPivotalAdvancedDatabaseServices1.1.4"><span style="color: rgb(0,0,0);line-height: 1.25;font-size: 24.0px;">Welcome to Pivotal Advanced Database Services 1.1.4</span></h1><p align="LEFT">Pivotal Advanced Database Services (ADS), extends Pivotal Hadoop (HD) Enterprise, adding rich, proven parallel SQL processing facilities. These SQL processing facilities enhance productivity, rendering Hadoop queries faster than any Hadoop-based query interface on the market. Pivotal ADS enables data analysis for a variety of Hadoop-based data formats using the Pivotal Extension Framework (PXF), without duplicating or converting HBase files. For the best performance, you can use an optimized format for Pivotal ADS table storage.</p><h2 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-AboutPADSComponents">About PADS Components</h2><p align="LEFT">PADS comprises the following components:</p><ul><li>HAWQ</li><li>PXF</li><li>MADlib</li></ul><h3 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-HAWQ">HAWQ</h3><p>HAWQ is a parallel SQL query engine that combines the key technological advantages of the industry-leading Greenplum Database with the scalability and convenience of Hadoop. HAWQ reads data from and writes data to HDFS natively.</p><p align="LEFT">Using HAWQ functionality, you can interact with petabyte range data sets. HAWQ provides users with a complete, standards compliant SQL interface.</p><p>Leveraging Greenplum Database’s parallel database technology, HAWQ consistently performs tens to hundreds of times faster than all Hadoop query engines in the market.</p><h3 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-PXF">PXF</h3><p>PXF enables SQL querying on data in the Hadoop components such as HBase, Hive, and any other distributed data file types. These queries execute in a single, zero materialization and fully-parallel workflow. PXF also uses the PADS advanced query optimizer and executor to run analytics on these external data sources, or transfers it to PADS to analyze locally. PXF connects Hadoop-based components to facilitate data joins, such as between HAWQ tables and HBase table. Additionally, the framework is designed for extensibility, so that user-defined connectors can provide parallel access to other data storage mechanisms and file types.</p><h4 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-PXFInteroperability">PXF Interoperability</h4><p>PXF operates as an integral part of PADS, and as a light add-on to Pivotal HD. On the database side, PXF leverages the external table custom protocol system. Therefore, creating a PXF table provides the same interoperability as an external table in Greenplum Database. On the Pivotal HD side, the PXF component physically lives on the Namenode and each or some Datanodes. It operates mostly as a separate service and does not interfere with Hadoop components internals.</p><h3 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-MADlib">MADlib<strong> </strong></h3><p>MADlib is an open-source library for scalable in-database analytics. It provides data-parallel implementations of mathematical, statistical and machine learning methods for structured and unstructured data. MADlib combines the efforts used in commercial practice, academic research, and open-source development. You can find more information at <u><a class="external-link" href="http://madlib.net" rel="nofollow">http://madlib.net</a></u>.</p><h1 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-NewFeatures">New Features</h1><p>This section describes the new features specific to PADS 1.1.4</p><p>For specific information about a previous release, please refer to the associated release notes.</p><h2 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-User-DefinedFunctions">User-Defined Functions</h2><p>PADS 1.1.3 provided support for User-Defined Functions (UDF). The UDF feature extends the functionality of the HAWQ database by providing functions that can be evaluated in SQL statements. With each release, Pivotal extends UDF feature support. This section lists the UDF functionality supported in each release, and explicitly calls out features that are not supported.</p><h3 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-UDFsupportinPADS1.1.4:">UDF support in PADS 1.1.4:</h3><ul><li>User defined aggregates</li><li>Procedural languages:<ul><li>PL/Python</li><li>PL/R</li></ul></li></ul><h3 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-UDFfeaturesnotsupportedinPADS1.1.4:">UDF features not supported in PADS 1.1.4:</h3><ul><li>SECURITY DEFINER when creating functions</li><li>Set returning functions</li><li>TABLE functions</li><li>Nested functions error out during execution</li><li>User-Defined base type</li><li>SORTOP</li><li>Window functions</li><li>ALTER set encoding, set schema, rename</li></ul><p>See the <em>HAWQ Installation Guide</em> for information about requirements and installation.</p><p>See the <em>Pivotal HAWQ Administrator Guide</em> for detailed information about features and usage.</p><h2 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-HAWQtoolkit">HAWQ toolkit</h2><p>The hawq_toolkit is an administrative schema that users can use to query system catalogs, log files, and operating environment for system status information. See the <em>Pivotal HAWQ Administrator Guide</em> for detailed information.</p><h1 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-AbouttheADS1.1.4Release">About the ADS 1.1.4 Release<span style="font-size: medium;"><span style="font-size: medium;"> </span></span></h1><p>This section lists the supported platforms and installation options for this release</p><h2 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-SupportedPlatforms">Supported Platforms</h2><p>PADS 1.1.4 supports the following platforms:</p><ul><li>Red Hat Enterprise 6.4-64 bit and 6.2-64 bit</li><li>CentOS 6.4-64 bit and 6.2-64 bit</li></ul><h2 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-Installationoptions">Installation options</h2><p>There are two ways to install HAWQ.</p><ul><li>Stand alone install – Please see <em>HAWQ 1.1 Installation Guide</em></li><li>Pivotal Command Center Command Line Interface– Please see <em>Pivotal HD Enterprise 1.0 Installation and Administrator Guide.</em></li></ul><h1 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-ResolvedIssuesinPADS1.1.4">Resolved Issues in PADS 1.1.4</h1><p>The table below lists issues that are now resolved in PADS 1.1.4.</p><p>For issues resolved in prior releases, refer to the corresponding release notes available from Support Zone</p><p><strong>Table: Resolved Issues in PADS 1.1.4</strong></p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">Issue</th><th class="confluenceTh">Category</th><th class="confluenceTh">Resolved in</th><th class="confluenceTh">Description</th></tr><tr><td class="confluenceTd">ARD-132</td><td class="confluenceTd">PXF</td><td class="confluenceTd">PADS 1.1.3</td><td class="confluenceTd"><p>During a join between two PXF tables, the optimizer may periodically hold the HBase side of the scan. In such a situation, the PXF HBase scanner timeout exception is not caught.<br/><strong>Workaround</strong><span> </span>: Increase the HBase scanner timeout.</p></td></tr><tr><td class="confluenceTd">HAWQ-1032</td><td class="confluenceTd">AO tables and Column Store</td><td class="confluenceTd">PADS 1.1.3</td><td class="confluenceTd"><p>If you inserted a large block of data, for example about 2M, in a single query, HAWQInputFormat will not be able to access it.<br/><strong>Workaround</strong><span style="font-size: xx-small;"> </span>: insert the data in multiple querieS.<span style="font-size: xx-small;"> </span></p><span style="font-size: xx-small;"> </span></td></tr><tr><td class="confluenceTd">HAWQ-1031</td><td class="confluenceTd">AO tables and Column Store</td><td class="confluenceTd">PADS 1.1.3</td><td class="confluenceTd">You will see the NullPointerException if you try to access a column using the wrong column name.</td></tr><tr><td class="confluenceTd">HAWQ-1023</td><td class="confluenceTd">HDFS Access LAyer </td><td class="confluenceTd">PADS 1.1.3 </td><td class="confluenceTd"><p>HAWQInputFormat throws the ClassCastException if it reads an invalid metadata file.</p> </td></tr><tr><td class="confluenceTd">HAWQ- 990 </td><td class="confluenceTd">AO tables and Column Store</td><td class="confluenceTd">PADS 1.1.3 </td><td class="confluenceTd">AOInputFormat: bytesTo DecimalStr throws the ArrayIndexOutOf BoundsException. Occurs when a table has columns with datatypes that are greater and less than 8 characters long. For example, the table may contain timestamps and int4. If all the 8-length columns in a tuple have a null value, HAWQInputFormat throws the ArrayIndexOutOfBoundsException exception.<span style="font-size: xx-small;"> </span></td></tr><tr><td class="confluenceTd">HD-2362</td><td class="confluenceTd"> PXF</td><td class="confluenceTd">PADS 1.1.1</td><td class="confluenceTd">The select query fails for an external table created on views for any HIVE table. A more comprehensive error message is now given.</td></tr><tr><td class="confluenceTd">HAWQ-282</td><td class="confluenceTd"> PXF</td><td class="confluenceTd">PADS 1.1.1</td><td class="confluenceTd"> The ANALYZE command silently ignored an incorrect Analyzer name when analyzing a PXF table. A WARNING is now issued.</td></tr><tr><td class="confluenceTd"><p>HAWQ-246</p><p> </p></td><td class="confluenceTd"> PXF</td><td class="confluenceTd">PADS 1.1.1</td><td class="confluenceTd"><p>A segmentdb could not read a block located on the same machine. The algorithm was enhanced to account for cases where PXF services are located on a remote region server.</p></td></tr><tr><td class="confluenceTd">HAWQ-245 </td><td class="confluenceTd"> PXF</td><td class="confluenceTd">PADS 1.1.1</td><td class="confluenceTd">Could not add PXF-specific logic to gp_external_max_segments. Modifying this parameters allows you to achieve the maximum possible distribution on hosts. You can also limit the number of segments that are running.<span style="font-size: xx-small;"> </span></td></tr><tr><td class="confluenceTd">HAWQ-216 </td><td class="confluenceTd">Management Tools </td><td class="confluenceTd">PADS1.1 </td><td class="confluenceTd"><p>gpstart failed if the cluster is busy. This issue did not happen frequently, since the system is idle at start time. However, it is important to remember that it could happen because the system may have other non-HAWQ workload on the same cluster.<span> </span></p></td></tr></tbody></table></div><h1 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-KnownIssuesinPADS1.1.x">Known Issues in PADS 1.1.x</h1><p>This section lists all the known issues in PADS 1.1.x. You will find issues and descriptions categorized under their respective components. A workaround is provided where applicable. This section contains the following topics:</p><ul><li>Known Issues in HAWQ 1.1.4.0</li><li>Known Issues in HAWQ 1.1.3.0</li><li>Known Issues in HAWQ 1.1.0.0</li><li>Known Issue in HAWQ 1.1.0.1</li><li>Known Issues in PXF 2.x.x</li></ul><p><strong>Important</strong>: Pivotal Hadoop (PHD) 1.1. has a new High Availability (HA) feature. This feature has the following known issue with HAWQ: If NameNode HA is configured in a cluster, HAWQ is unable to take advantage of HA capability when the primary configure HA for HDFS. In the case of the primary NameNode failing, HAWQ will not be able to failover to the secondary NameNode. Manual re-direction to secondary NameNode is not currently supported in this release. Therefore we highly recommend that HA should be disabled when used with HAWQ.</p><h2 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-KnownIssuesinHAWQ1.1.4.0">Known Issues in HAWQ 1.1.4.0</h2><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">Issue</th><th class="confluenceTh">Category</th><th class="confluenceTh">Description</th></tr><tr><td class="confluenceTd" colspan="1">HAWQ-1610</td><td class="confluenceTd" colspan="1">Build and Installer </td><td class="confluenceTd" colspan="1"><p>PL/R package changes.</p><p>Check the name of your plr package. If it is plr-1.1.4.0-5152.x86_64.tgz,download the latest version plr-1.1.4.0-5664.x86_64.tgz for HAWQ 1.1.4.0 from Pivotal. The new package contains the file plr.sql with the necessary PL/R helper functions.</p></td></tr><tr><td class="confluenceTd" colspan="1">HAWQ-1418 </td><td class="confluenceTd" colspan="1">Catalog and Metadata</td><td class="confluenceTd" colspan="1"> HAWQ 1.1.4.0 does not support aggregate derived functions. </td></tr><tr><td class="confluenceTd">HAWQ-1260 </td><td class="confluenceTd"> Query Execution</td><td class="confluenceTd"><p>A certain class of uncorrelated subqueries are known to fail. The subquery should have a user defined object and a distributed table. For example:</p><p>SELECT * FROM t1 WHERE t1.a &lt; (SELECT foo(t2.b) FROM t2 LIMIT 1);</p><p>In this example, the subquery "SELECT foo(t2.b) FROM t2 LIMIT 1" has no correlation with the outer query.  The subquery also invokes the UDF "foo()" and queries a distributed table "t2".  Another example:</p><p>SELECT array(SELECT foo(t1.a) FROM t1);</p><p>Such type of queries fail with the following error:</p><p>ERROR cache lookedup failed for ...</p></td></tr><tr><td class="confluenceTd">HAWQ-1379</td><td class="confluenceTd">Management Tool</td><td class="confluenceTd"><p>hawq_toolkit cannot be used directly after upgrading from an old version. This is because toolkit related objects are not created in the old version.</p><p><strong>Workaround</strong>: for each existing database instance where a user wants to use hawq_toolkit, perform following steps as superuser:</p><ol><li>create a new schema named hawq_toolkit: <span style="color: rgb(34,34,34);">psql -q -c "CREATE SCHEMA hawq_toolkit" $DATABASE_NAME</span></li><li><span style="color: rgb(34,34,34);">create toolkit related objects: <span style="color: rgb(34,34,34);">psql -q -f $INSTALL_DIR/share/postgresql/gp_toolkit.sql $DATABASE_NAME</span></span><span style="color: rgb(34,34,34);"><span style="color: rgb(34,34,34);"><br/></span></span></li></ol><p><span style="color: rgb(34,34,34);"><span style="color: rgb(34,34,34);"><span style="color: rgb(34,34,34);">After performing the above operations on template1, every newly created database using template1 as template database, will have hawq_toolkit automatically, meaning no need to perform the above operation.</span><br/></span></span></p></td></tr><tr><td class="confluenceTd">HAWQ-1368</td><td class="confluenceTd">Management Tool</td><td class="confluenceTd">The view, hawq_size_of_database, does not check user permission of those databases and only reports sizes of all user databases.</td></tr><tr><td class="confluenceTd" colspan="1">HAWQ-1270</td><td class="confluenceTd" colspan="1">Management Tool</td><td class="confluenceTd" colspan="1">The user must have access permission to the view, hawq_size_of_schema_disk.</td></tr><tr><td class="confluenceTd" colspan="1">HAWQ-1369</td><td class="confluenceTd" colspan="1">Management Tool</td><td class="confluenceTd" colspan="1">When the underlying HDFS is online, hawq_size_of_database includes the data size on both HDFS and local storage of the master; when the HDFS is offline, that view only has the data size on local storage of the master.</td></tr></tbody></table></div><h2 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-KnownIssuesinHAWQ1.1.3.0">Known Issues in HAWQ 1.1.3.0</h2><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">Issue</th><th class="confluenceTh">Category</th><th class="confluenceTh">Description</th></tr><tr><td class="confluenceTd"><p>HAWQ-1167</p></td><td class="confluenceTd">Performance</td><td class="confluenceTd"><p>Enabling Kerberos shows a 10% downgrade in HAWQ performance.</p></td></tr><tr><td class="confluenceTd">HAWQ-1099</td><td class="confluenceTd">Connectivity</td><td class="confluenceTd"><p>If you enable kerberos authentication, the ODBC function SQL GetInfo returns an incorrect version of HAWQ.</p></td></tr><tr><td class="confluenceTd">HAWQ-1078</td><td class="confluenceTd">Query Execution</td><td class="confluenceTd"><p>Continuously issue deepslice queries cause error in HDFS with kerberos.</p></td></tr><tr><td class="confluenceTd">HAWQ-1056</td><td class="confluenceTd">DML</td><td class="confluenceTd"><p>Inserting data into a temp table generates an Append-only Storage Write error.</p></td></tr><tr><td class="confluenceTd">HAWQ-1022</td><td class="confluenceTd">Utility commands</td><td class="confluenceTd"><p>Default value for dfs.client.socket-timeout in HDFS setting in gpcheck need to be corrected.</p></td></tr></tbody></table></div><h2 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-KnownIssuesinHAWQ1.1.0.3">Known Issues in HAWQ 1.1.0.3</h2><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">Issue</th><th class="confluenceTh">Category</th><th class="confluenceTh">Description</th></tr><tr><td class="confluenceTd"><p>HAWQ-859</p></td><td class="confluenceTd">Query Optimizer</td><td class="confluenceTd"><p>pg_dumpall test suite runs slowly</p><p>The overhead is due to the command pg_dumpall. pg_dumpall generates multiple queries over the catalog tables. Since ORCA optimizes these queries. Although these are simple queries, ORCA adds the overhead.</p><p><strong>Workaround</strong><span style="font-size: xx-small;"> </span>: Turn ORCA off.</p></td></tr></tbody></table></div><h2 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-KnownIssuesinHAWQ1.1.0.1">Known Issues in HAWQ 1.1.0.1</h2><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">Issue</th><th class="confluenceTh">Category</th><th class="confluenceTh">Description</th></tr><tr><td class="confluenceTd"><p>HAWQ-256</p></td><td class="confluenceTd">Storage</td><td class="confluenceTd"><p>HAWQ does not support kerberos authentication on HDFS.</p></td></tr><tr><td class="confluenceTd">HAWQ-255</td><td class="confluenceTd">Network</td><td class="confluenceTd"><p>HAWQ does not support the IPv6 protocol.</p></td></tr><tr><td class="confluenceTd">HAWQ-225</td><td class="confluenceTd">Storage</td><td class="confluenceTd"><p>When the number of partitions or columns of a column oriented table is large or write concurrency is high, HAWQ encounters an HDFS concurrency write limitation. Data loading performance may degrade and fail.</p><p><strong>Workaround: </strong><span style="font-size: xx-small;"> </span>for partitioned tables, load data partitions one by one, instead of loading all the data randomly to all the partitions.</p></td></tr><tr><td class="confluenceTd">HAWQ-224</td><td class="confluenceTd">Backup and Restore</td><td class="confluenceTd"><p><span style="font-size: xx-small;"> </span>Only non-parallel logical backup and restore is supported. Pivotal recommends that you use physical backup and restore.</p></td></tr><tr><td class="confluenceTd">HAWQ-26</td><td class="confluenceTd">DDL</td><td class="confluenceTd"><p>duplicate key violates unique constraint pg_type_typname_nsp_indexWhen two sessions attempt to create a table with the same name and in the same namespace, one of the sessions will error out with a less user-friendly error message of the form "duplicate key violates unique constraint".</p></td></tr></tbody></table></div><h2 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-KnownIssuesinPXF2.x.x">Known Issues in PXF 2.x.x</h2><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">Issue</th><th class="confluenceTh">Category</th><th class="confluenceTh">Description</th></tr><tr><td class="confluenceTd" colspan="1">HAWQ-1394 </td><td class="confluenceTd" colspan="1">PXF 2.1.1</td><td class="confluenceTd" colspan="1"><p>When using PXF to communicate with a kerberized Pivotal Hadoop, PXF assumes that P-HD is using port 8020. If that is not the case, PXF will fail to communicate and transfer data. You will see the following message:</p><p>ERROR:  fail to get filesystem credential for uri hdfs://&lt;namenode&gt;:8020/ (cdbfilesystemcredential.c:194)</p></td></tr><tr><td class="confluenceTd" colspan="1"> HAWQ-1380</td><td class="confluenceTd" colspan="1">PXF 2.1.1 </td><td class="confluenceTd" colspan="1"><p>Query conditions specified with more than one element are not pushed down to PXF.</p><p>No workaround is necessary since the query is filtered by HAWQ and returns the correct results. This behavior only impacts the performance of HBase and Hive.</p></td></tr><tr><td class="confluenceTd"><p>HAWQ-1142</p></td><td class="confluenceTd">PXF 2.0.3</td><td class="confluenceTd"><p>If you write the ANALYZE command on PXF writable tables it fails with an error.</p><p><strong>Workaround</strong><span style="font-size: xx-small;"> </span>: Add FRAGMENTER or PROFILE parameters to the location of the PXF writable table. The ANALYZE command will issue a warning, but will not fail.<span style="font-size: xx-small;"> </span> </p></td></tr><tr><td class="confluenceTd">HAWQ-1133</td><td class="confluenceTd">PXF 2.0.3</td><td class="confluenceTd"><p>A query might fail if the same DN has to read several BZip2 jobs at the same time. This is caused because BZip2 codec is not thread safe.</p><p><strong>Workaround</strong><span style="font-size: xx-small;"> </span>: Pivotal recommends that you do not read more than one BZip2 files at the same time.</p></td></tr><tr><td class="confluenceTd">HAWQ-1111</td><td class="confluenceTd">PXF 2.0.3</td><td class="confluenceTd"><p>HAWQ does not report the details of a BadRecordException error message generated in PXF.</p><p><strong>Workaround</strong><span> </span>: You must look for the details in the PXF log on HDFS.</p></td></tr><tr><td class="confluenceTd">HAWQ-1141</td><td class="confluenceTd">PXF 2.0.2</td><td class="confluenceTd"><p>If the rows in a Hive array are in different sizes, HiveResolver returns incorrect data.</p></td></tr><tr><td class="confluenceTd" colspan="1">HD-6122 </td><td class="confluenceTd" colspan="1">PXF 2.0.2</td><td class="confluenceTd" colspan="1"><p>PXF only works with HiveServer1, not HiveServer2.</p> </td></tr><tr><td class="confluenceTd" colspan="1">HAWQ-291 </td><td class="confluenceTd" colspan="1">PXF 2.0.1 </td><td class="confluenceTd" colspan="1">HDFS does not work properly when accessing data files that contain header rows.<p><strong>Workaround</strong><span style="font-size: xx-small;"> </span>: Specify an error table with the "no HEADER" flag.</p></td></tr></tbody></table></div><h1 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-PivotalandPADSInteroperability">Pivotal and PADS Interoperability</h1><p>Pivotal releases a number of client tool packages on various platforms that can be used to connect to PADS. The following table describes the client tool package compatibility with PADS. Client tool packages are available at the EMC Download Center.</p><p><strong>Table: Interoperability Matrix</strong></p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh"> Client package</th><th class="confluenceTh">Description </th><th class="confluenceTh">Operating system </th><th class="confluenceTh">Client version</th><th class="confluenceTh">HAWQ version </th></tr><tr><td class="confluenceTd">Connectivity</td><td class="confluenceTd">Standard PostgreSQL Database Drivers (ODBC, JDBC)</td><td class="confluenceTd">Windows 2008 RedHat 6.4 and 6.2, 64 bit</td><td class="confluenceTd">4.2.6SP </td><td class="confluenceTd">1.1.4 </td></tr><tr><td class="confluenceTd"><p>HAWQ Client</p></td><td class="confluenceTd">Command Line Interface</td><td class="confluenceTd">Windows 2008 RedHat 6.4 and 6.2, 64 bit</td><td class="confluenceTd"> 4.2.6SP </td><td class="confluenceTd">1.1.4</td></tr><tr><td class="confluenceTd">Pivotal Command Center </td><td class="confluenceTd"><p>A web-based tool for managing and monitoring your Pivotal HD cluster.</p><p align="LEFT">Note: Pivotal Command Center 2.0.x does not support DCA V1, DCA V2 or Greenplum Database.</p></td><td class="confluenceTd"><p>Windows 2008 RedHat 6.4 and 6.2, 64 bit </p><p>CentOS 6.4 and 6.2, 64 bit</p></td><td class="confluenceTd">2.1.1 </td><td class="confluenceTd">1.1.4</td></tr><tr><td class="confluenceTd">PXF </td><td class="confluenceTd"><p>Extensibility layer to provide support for external data formats such as HBase and Hive.</p> </td><td class="confluenceTd"><p>Windows 2008 RedHat 6.4 and 6.2, 64 bit </p><p>CentOS 6.4 and 6.2, 64 bit</p> </td><td class="confluenceTd">2.1.1</td><td class="confluenceTd">1.1.4 </td></tr><tr><td class="confluenceTd" colspan="1">Pivotal HD</td><td class="confluenceTd" colspan="1"> Pivotal Hadoop</td><td class="confluenceTd" colspan="1"><p>Windows 2008 RedHat 6.4 and 6.2, 64 bit </p><p>CentOS 6.4 and 6.2, 64 bit</p> </td><td class="confluenceTd" colspan="1">1.1.1</td><td class="confluenceTd" colspan="1">1.1.4 </td></tr><tr><td class="confluenceTd" colspan="1">Pivotal Data Loader </td><td class="confluenceTd" colspan="1"><p>Data Loader is a management tool that loads data to distributed data analytics platforms such as Hadoop, and HAWQ.</p> </td><td class="confluenceTd" colspan="1"><p>Windows 2008 RedHat 6.4 and 6.2, 64 bit </p><p>CentOS 6.4 and 6.2, 64 bit</p> </td><td class="confluenceTd" colspan="1">2.0.4</td><td class="confluenceTd" colspan="1">1.1.4 </td></tr></tbody></table></div><h1 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-PADS1.1.4andPivotalHDDocumentation">PADS 1.1.4 and Pivotal HD Documentation</h1><p>The following PADS and related documentation is available in PDF format on our website at <a class="external-link" href="http://www.gopivotal.com" rel="nofollow">www.gopivotal.com</a>.</p><p>HTML versions of our documentation is available here: docs.gopivotal.com/pivotalhd/</p><p>Additionally, you can still access product documentation from EMC's <a class="external-link" href="https://support.emc.com/" rel="nofollow">Support Zone</a>.</p><p><strong>Table: HAWQ Documentation</strong></p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">Title</th><th class="confluenceTh">Revision</th></tr><tr><td class="confluenceTd"><p>Pivotal ADS 1.1.4 Release Notes (This document)</p></td><td class="confluenceTd">A02</td></tr><tr><td class="confluenceTd"><p>Pivotal HAWQ 1.1 Installation Guide</p></td><td class="confluenceTd">A11</td></tr><tr><td class="confluenceTd"><p>Pivotal HAWQ 1.1 Administrator Guide</p></td><td class="confluenceTd">A08</td></tr><tr><td class="confluenceTd"><p>Pivotal HD Enterprise 1.1 Installation and Administrator Guide</p></td><td class="confluenceTd">A03</td></tr><tr><td class="confluenceTd"><p>Pivotal HD DataLoader 2.0 Installation and User Guide</p></td><td class="confluenceTd">A06</td></tr><tr><td class="confluenceTd"><p>Pivotal HD 1.1 Stack and Tool Reference Guide</p></td><td class="confluenceTd">A02</td></tr><tr><td class="confluenceTd"><p>Pivotal Command Center 2.1 User Guide</p></td><td class="confluenceTd">A02</td></tr><tr><td class="confluenceTd"><p>Pivotal Extension Framework Installation and User Guide</p></td><td class="confluenceTd">A02</td></tr></tbody></table></div><h1 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-UpgradingtoHAWQ1.1.x">Upgrading to HAWQ 1.1.x</h1><p>The upgrade path supported for this release is HAWQ 1.0.x to HAWQ 1.1.x.</p><p align="LEFT">For detailed upgrade procedures and information, see the following sections:</p><ul><li>Upgrading from HAWQ 1.1.x to HAWQ 1.1.y</li><li>Upgrading from 1.0.x to HAWQ 1.1.x</li></ul><p align="LEFT"><strong>Note: </strong><span style="font-size: xx-small;"> </span>Pivotal recommends that you back up any existing data before upgrading to HAWQ1.1.x.</p><p><strong>Note</strong>:Follow these instructions if you installed HAWQ manually. To upgrade PHD Manager, see the Pivotal HD Enterprise 1.0 Installation and Administration Guide.</p><h2 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-UpgradingfromHAWQ1.1.xtoHAWQ1.1.y">Upgrading from HAWQ 1.1.x to HAWQ 1.1.y</h2><p>An upgrade from HAWQ 1.1.x to HAWQ 1.1.y involves stopping HAWQ, updating the HAWQ software binaries, and restarting HAWQ.</p><p>Log in to your HAWQ master host as the HAWQ administrative user:</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ su - gpadmin</pre>
</div></div><p>Perform a smart shutdown of your current HAWQ 1.1.x system (shut down all active connections to the database):</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ gpstop</pre>
</div></div><p>Run the installer for 1.1.y on the HAWQ master host using rpm. This installs HAWQ to /usr/local/hawq-1.1.y alongside any older versions, and it will point a soft link from /usr/local/hawq to /usr/local/hawq-1.1.y.</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ su - root # rpm -ivh hawq-1.1.y.x86_64.rpm --force</pre>
</div></div><p>Run the following command to install the HAWQ 1.1.y binaries on all the hosts specified in the <em>hostfile</em><span style="font-size: medium;"> </span>:</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;"># gpssh -f hostfile -e "rpm -ivh hawq-1.1.y.x86_64.rpm --force"</pre>
</div></div><p align="LEFT">After all segment hosts have been upgraded, you can log in as gpadmin user and restart your HAWQ system:</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ su - gpadmin
$ gpstart </pre>
</div></div><h2 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-UpgradingfromHAWQ1.0.xtoHAWQ1.1.x">Upgrading from HAWQ 1.0.x to HAWQ 1.1.x</h2><p>This section describes how you can upgrade from HAWQ 1.0.x or later to HAWQ 1.1.x.</p><p>This section divides the upgrade into the following phases: pre-upgrade preparation, software installation, upgrade execution, and post-upgrade tasks.</p><p><strong>Important</strong>: Carefully evaluate each section and perform all the required and conditional steps. Failing to perform any of these steps can result in a aborted upgrade, placing your system in an unusable or even unrecoverable state.</p><h2 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-PreupgradePreparation">Preupgrade Preparation</h2><p>Perform these steps on your current HAWQ system. This procedure is performed from your HAWQ master host and should be executed by the HAWQ superuser (gpadmin).</p><p align="LEFT">Log in to the HAWQ master as the gpadmin user</p><p align="LEFT">(<em>optional</em><span style="font-size: medium;"> </span>) Vacuum all databases prior to upgrade. For example:</p><p align="LEFT">(<em>optional</em><span style="font-size: medium;"> </span>) Clean out old server log files from your master and segment data directories. For example, to remove log files from 2011 from your segment hosts:</p><p align="LEFT">Note: Running Vacuum and cleaning out old logs files is not required, but it will reduce the size of HAWQ files to be backed up and migrated.</p><p align="LEFT">Run gpstate to check for failed segments.</p><p align="LEFT">If you have failed segments, you must recover them using gprecoverseg before you can upgrade.</p><p align="LEFT">Copy or preserve any additional folders or files (such as backup folders) that you have added in the HAWQ data directories or $GPHOME directory. Only files or folders strictly related to HAWQ operations are preserved by the migration utility.</p><h2 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-InstalltheHAWQSoftwareBinaries">Install the HAWQ Software Binaries</h2><p align="LEFT">Run the installer for 1.1.x on the HAWQ master host using rpm. This installs HAWQ to /usr/local/hawq-1.1.x alongside any older versions, and it will point a soft link from /usr/local/hawq to /usr/local/hawq-1.1.x.</p><p align="LEFT">Run the following command to then install the HAWQ 1.1.x binaries on all the hosts specified in the <em>hostfile.</em></p><h1 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-UpgradeExecution">Upgrade Execution</h1><p align="LEFT">During upgrade, all client connections to the master are locked out. Before performing this procedure, inform all database users of the upgrade and lockout time frame. From this point onward, be sure that no one is on the system until the upgrade is complete.</p><ol><li><p>Source the path file from your old 1.0.x installation. For example:</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ source /usr/local/hawq-1.0.x/greenplum_path.sh</pre>
</div></div></li><li><p>If your system has a standby master host configured, remove the standby master from your system configuration. For example:</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ gpinitstandby -r</pre>
</div></div></li><li><p>Perform a clean shutdown of your current ADS system. For example:</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ gpstop</pre>
</div></div></li><li><p>Source the path file from your new HAWQ 1.1.x installation. For example:</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ source /usr/local/hawq-1.1.x/greenplum_path.sh</pre>
</div></div></li><li><p>As gpadmin, run the 1.1.x version of the migration utility specifying your old and new GPHOME locations. If your system does not have mirrors, use gpmigrator. For example on a system with mirrors:</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ su - gpadmn
$ gpmigrator /usr/local/hawq-1.0.x /usr/local/hawq-1.1.x</pre>
</div></div><p>Note: If the migration does not complete successfully, contact Customer Support. See Troubleshooting a Failed Upgrade.</p></li><li>The migration can take a while to complete. After the migration utility has completed successfully, the HAWQ 1.1.x system will be running and accepting connections.</li></ol><h2 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-PostUpgradeonyourHAWQ1.1.xSystem">Post Upgrade on your HAWQ 1.1.x System</h2><ol><li><p>If your system had a standby master host configured, reinitialize your standby master using gpinitstandby:</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ gpinitstandby -s standby_hostname</pre>
</div></div></li><li>If your system uses external tables with gpfdist, stop all gpfdist processes on your ETL servers and reinstall gpfdist using the compatible HAWQ 1.1.x Load Tools package. Application Packages are available at the EMC Download Center.</li><li>If you want to use the Pivotal Command Center management tool, install the latest Command Center Console. To update your environment variable to point to the latest Command Center binaries, source the gpperfmon_path.sh file from your new installation.</li></ol><p align="LEFT" style="margin-left: 30.0px;">Note: The Pivotal Command Center management tool replaces Greenplum command Center and Greenplum Performance Monitor. Command Center Console packages are available from the EMC Download Center.</p><p align="LEFT">      4. If you had created tables using GPXF, you need to DROP these tables and CREATE them again using PXF 2.0.1.</p><p align="LEFT" style="margin-left: 30.0px;">  See Appendix G, Pivotal Extension Framework, in the Pivotal ADS 1.1 Administrator Guide, A02, for instructions about how to install PXF 2.0.1.</p><p align="LEFT" style="margin-left: 30.0px;">Note: When you CREATE the tables for PXF2.0.1, remember to perform the following:</p><p align="LEFT" style="margin-left: 60.0px;">a. Change the protocol name in the LOCATION clause from gpxf to pxf.</p><p align="LEFT" style="margin-left: 60.0px;">b. Ensure that Fragmenter, Accessor, and Resolver are <em>always </em><span style="font-size: medium;"> </span>specified for the table.</p><p align="LEFT" style="margin-left: 60.0px;">c. Manually recreate the two symbolic links that are removed during the update:</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">cd /usr/lib/gphd # base directory for PHD
mkdir publicstage
chown hdfs /usr/lib/gphd/publicstage
chmod 777 /usr/lib/gphd/publicstage
ln -s gpxf-x.x.x gpxf</pre>
</div></div><p align="LEFT">               d. Check that you have the new names for the Fragmenter, Accessor, and Resolver classes.</p><p style="margin-left: 60.0px;">    See Appendix G, Pivotal Extension Framework, in the <em>PADS 1.1 Administrator Guide</em><span style="font-size: medium;"> </span>, A02, for information about the Java classes.</p><p align="LEFT">               e. Check that you are using the correct gucs for PXF:</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">gpxf_enable_filter_pushdown -&gt; pxf_enable_filter_pushdown
gpxf_enable_stat_collection -&gt; pxf_enable_stat_collection
gpxf_enable_locality_optimizations -&gt; pxf_enable_locality_optimizations</pre>
</div></div><p align="LEFT">        5. Use the pgcrypto package from 1.1.3 distribution and reinstall pgcrypto after the upgrade is done. Follow the instructions in the HAWQ Installation Guide to install pgcrypto.</p><p align="LEFT">        6. Install the PL/R language package according to instructions in the <em>HAWQ Installation Guide</em>. </p><p align="LEFT">        7. Inform all database users of the completed upgrade. Tell users to update their environment to source the HAWQ 1.1.x installation (if necessary).</p><h1 id="PivotalAdvancedDatabaseServices1.xReleaseNotes-TroubleshootingaFailedUpgrade">Troubleshooting a Failed Upgrade</h1><p align="LEFT">If you experience issues during the migration process, go to the Support page at Support Zone or contact Pivotal customer support at one of the following numbers:</p><ul><li>United States: 800-782-4362 (1-800-SVC-4EMC)</li><li>Canada: 800-543-4782</li><li>Worldwide: +1-508-497-7901</li></ul><p align="LEFT">Be prepared to provide the following information:</p><ul><li>A detailed list of upgrade procedures completed.</li><li>Log output from gpmigrator (located in ~/gpAdminLogs).<br/><span style="font-size: medium;"><span style="font-size: medium;"> </span></span></li></ul><p align="LEFT"><span style="font-size: medium;"> </span></p><p align="LEFT"><span style="font-size: small;"><span style="font-size: medium;"> </span></span></p><p align="LEFT"><span style="font-size: medium;"> </span></p><p align="LEFT"><span style="font-size: small;"> </span></p><p align="LEFT"><span style="font-size: medium;"> </span></p><p align="LEFT"><span style="font-size: medium;"> </span></p><p align="LEFT"><span style="font-size: medium;"><span style="font-size: medium;"><span style="font-size: medium;"><span style="font-size: medium;"><span style="font-size: medium;"> </span></span></span></span></span></p><p align="LEFT"><span style="font-size: medium;"> </span></p><p align="LEFT"><span style="font-size: medium;"> </span></p><p align="LEFT"> </p><p align="LEFT"><span style="font-size: medium;"><strong><span style="font-size: xx-small;"> </span></strong><span style="font-size: xx-small;"> </span></span></p><p align="LEFT"><span style="font-size: medium;"> </span></p><p><span style="font-size: medium;"> </span></p><p><span style="font-size: medium;"> </span></p><p> </p><p><span style="font-size: medium;"> </span></p><p><span style="font-size: medium;"><span style="font-size: medium;"> </span></span></p><p>.</p><p><span style="font-size: medium;"> </span></p><p> </p><p> </p><p> </p>
</div></div>


            </div><!-- end of body-container content-->
          </div><!-- end of container -->
        </div><!--end of container-fluid-->
      </div><!--end of main-wrap-->

      <div class="site-footer desktop-only">
          <div class="container-fluid">
              <div class="site-footer-links">
                  <span class="version"><a href='/'>Pivotal Documentation</a></span>
                  <span>&copy;
                      <script>
                          var d = new Date();
                          document.write(d.getFullYear());
                      </script>
                      <a href='http://gopivotal.com'>Pivotal Software</a> Inc. All Rights Reserved.
                  </span>
              </div>
          </div>
      </div>

      <script type="text/javascript">
          (function() {
              var didInit = false;
              function initMunchkin() {
                  if(didInit === false) {
                      didInit = true;
                      Munchkin.init('625-IUJ-009');
                  }
              }
              var s = document.createElement('script');
              s.type = 'text/javascript';
              s.async = true;
              s.src = document.location.protocol + '//munchkin.marketo.net/munchkin.js';
              s.onreadystatechange = function() {
                  if (this.readyState == 'complete' || this.readyState == 'loaded') {
                      initMunchkin();
                  }
              };
              s.onload = initMunchkin;
              document.getElementsByTagName('head')[0].appendChild(s);
          })();
      </script>
  </div><!--end of viewport-->
  <div id="scrim"></div>
</body>
</html>